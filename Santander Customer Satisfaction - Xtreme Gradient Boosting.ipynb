{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB- Feature selection and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the packages for modeling\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperate out predictors and target from the training data set\n",
    "# Remove the ID field from the test dataset and save it.\n",
    "# Drop the ID field from the training set\n",
    "train_y = train['TARGET']\n",
    "train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "train_x = train\n",
    "test_id = test['ID']\n",
    "del test['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation \n",
    "### Remove duplicate and constant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixing the outliers in column 'var3'\n",
    "train_x['var3'].replace(-999999,0, inplace=True)\n",
    "test['var3'].replace(-999999,0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all the columns which have constant values. \n",
    "# These columns have zero std deviation.\n",
    "rm_col=[] \n",
    "for col in train_x.columns:\n",
    "    if train_x[col].std()==0:\n",
    "        rm_col.append(col)\n",
    "\n",
    "train_x.drop(rm_col, axis=1, inplace=True)\n",
    "test.drop(rm_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the duplicate columns. \n",
    "# Here we have columns with different name but exactly same values for each rows\n",
    "# We will compare all pairs of columns\n",
    "dups_col = []\n",
    "for ii in range(len(train_x.columns)-1):\n",
    "    for jj in range(ii+1,len(train_x.columns)):\n",
    "        col1=train_x.columns[ii]\n",
    "        col2=train_x.columns[jj]\n",
    "        # take the columns as arrays adn then compare the values.\n",
    "        if np.array_equal(train_x[col1].values, train_x[col2].values) and not col2 in dups_col:\n",
    "            dups_col.append(col2)\n",
    "\n",
    "train_x.drop(dups_col, axis=1, inplace=True)\n",
    "test.drop(dups_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using XGBoost classifier\n",
    "#### We will leverage the feature importance attribute of the XGBoost  classifier to find top 50 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define XGBoost classifier with some standard parameter settings\n",
    "xgb_clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1,\n",
    "                           gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic',\n",
    "                           nthread=4,seed=10)\n",
    "\n",
    "# Learn the model with training data\n",
    "xgb_clf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the top 50 important features\n",
    "imp_feat_xgb=pd.Series(xgb_clf.feature_importances_, index=train_x.columns).sort_values(ascending=False)\n",
    "imp_feat_xgb[:50].plot(kind='bar',title='Top 50 Important features as per XGBoost', figsize=(12,8))\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "plt.savefig('FeatureImportance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save indexes of the important features in descending order of their importance\n",
    "indices = np.argsort(xgb_clf.feature_importances_)[::-1]\n",
    "\n",
    "# list the names of the names of top 50 selected features adn remove the unicode\n",
    "select_feat =[str(s) for s in train_x.columns[indices][:50]]\n",
    "\n",
    "# Make the subsets with 50 features only\n",
    "train_x_sub = train_x[select_feat]\n",
    "test_sub = test[select_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "#### We will use GridSearch package with cross validation to find best setting of the parameters from a range of values\n",
    "\n",
    "#### We will tune the parameters in multiple rounds. At each round, we we will take 1 or 2 parameters find their best values and set them in next step for tuning different set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1: Tune max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new XGBoost Classifier with default parameters\n",
    "select_xgb_clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, seed=10)\n",
    "\n",
    "# Set a list of parameters\n",
    "param_grid = {\n",
    "    \n",
    "            'max_depth':[3,4,5],\n",
    "            'min_child_weight':[3,4,5]\n",
    "}\n",
    "grid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5,scoring='roc_auc' )\n",
    "\n",
    "# Train the model\n",
    "grid_clf.fit(train_x_sub,train_y)\n",
    "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2: Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new XGBoost Classifier setting the best value for the above parameter and \n",
    "# default for the rest\n",
    "select_xgb_clf = xgb.XGBClassifier(learning_rate=0.1,n_estimators=100, max_depth= 5, \n",
    "                                   min_child_weight= 5,seed=10)\n",
    "\n",
    "# Set a list of parameters\n",
    "param_grid = {\n",
    "    \n",
    "            'subsample':[0.6,0.7,0.8,0.9],\n",
    "            'colsample_bytree':[0.6,0.7,0.8,0.9]\n",
    "}\n",
    "grid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n",
    "\n",
    "# Train the model\n",
    "grid_clf.fit(train_x_sub,train_y)\n",
    "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3:  Tune reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new XGBoost Classifier with parameters setting so far.\n",
    "select_xgb_clf = xgb.XGBClassifier(learning_rate=0.1,n_estimators=100, max_depth= 5, min_child_weight= 5,\n",
    "                                   gamma=0,subsample=0.7, colsample_bytree=0.7, seed=10)\n",
    "\n",
    "# Set a list of parameters\n",
    "param_grid = {\n",
    "    \n",
    "         'reg_alpha':[0.001, 0.005, 0.01, 0.05]   \n",
    "}\n",
    "grid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n",
    "\n",
    "# Train the model\n",
    "grid_clf.fit(train_x_sub,train_y)\n",
    "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 4: Tune learning_rate\n",
    "#### We will take values on both sides of the default learning rate (0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have run the below code multiple times with best the learning rate and changing the n_estimators. \n",
    "#### It seems 75 gives me the best score so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new XGBoost Classifier.\n",
    "select_xgb_clf = xgb.XGBClassifier(n_estimators=75, max_depth= 5, min_child_weight= 5,gamma=0,\n",
    "                                   reg_alpha= 0.01,subsample=0.7, colsample_bytree=0.7, seed=10)\n",
    "\n",
    "# Set a list of parameters\n",
    "param_grid = {\n",
    "    \n",
    "         'learning_rate':[0.05,0.08, 0.1, 0.15]   \n",
    "}\n",
    "grid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n",
    "\n",
    "# Train the model\n",
    "grid_clf.fit(train_x_sub,train_y)\n",
    "grid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take teh best model from the grid search\n",
    "best_xgb_clf = grid_clf.best_estimator_\n",
    "grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make prediction with test data\n",
    "predicted_proba = best_xgb_clf.predict_proba(test_sub)\n",
    "\n",
    "# Save the prediction in CSV file\n",
    "# predicted_proba has probabilities for each Target class for each observation.\n",
    "# We are concerned about probability of class 1 and hence taking predicted_proba[:,1]\n",
    "submission = pd.DataFrame({'ID':test_id,'TARGET':predicted_proba[:,1]})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
